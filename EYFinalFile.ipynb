{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01fd38b5-7e0c-4127-8bfe-5caa44007daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing and loading the essential libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import rasterio\n",
    "from rasterio.transform import rowcol\n",
    "from scipy.stats import zscore\n",
    "from scipy.stats import randint\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bac9d17-01bf-4aab-8453-63e4c3c74326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location where the tiff file that contains all climate varibles for each loaction is available \n",
    "tiff_path = \"/Users/vinayakshanbhag/Downloads/TerraClimate_output.tiff\"\n",
    "\n",
    "# Function to load climate data of a dataset containing latitude and longitude \n",
    "# This is used to extract climate variables for both training set and validation set\n",
    "def extract_climate(df, tiff_path):\n",
    "    with rasterio.open(tiff_path) as src:\n",
    "        bands = [src.read(i) for i in range(1, src.count + 1)]\n",
    "        extracted = []\n",
    "        for _, row in df.iterrows():\n",
    "            lon, lat = row[\"Longitude\"], row[\"Latitude\"]\n",
    "            try:\n",
    "                r, c = rowcol(src.transform, lon, lat)\n",
    "                pixel_vals = [bands[b][r, c] for b in range(len(bands))]\n",
    "            except:\n",
    "                pixel_vals = [np.nan] * len(bands)\n",
    "            extracted.append(pixel_vals)\n",
    "\n",
    "    climate_vars = ['tmin', 'tmax', 'vap', 'ppt', 'srad', 'ws',\n",
    "                    'aet', 'pet', 'q', 'def', 'soil', 'swe', 'pdsi', 'vpd']\n",
    "    climate_df = pd.DataFrame(extracted, columns=climate_vars)\n",
    "    climate_df[\"Latitude\"] = df[\"Latitude\"].values\n",
    "    climate_df[\"Longitude\"] = df[\"Longitude\"].values\n",
    "    return climate_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84f7dfea-280e-4218-a07a-712992e7dfcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Climate (Training) Data Shape: (6312, 16)\n",
      "        tmin        tmax  vap         ppt  srad   ws        aet         pet  \\\n",
      "0  53.500000   65.300003 -4.5  115.500000  49.7  2.5  16.700001  200.799149   \n",
      "1  24.800001  110.800003 -3.9  143.100006  26.0  1.3   2.500000  217.699799   \n",
      "2  51.299999   28.200001 -3.8  115.099998  69.9  3.5  68.800003  204.000031   \n",
      "3  41.000000   67.300003 -4.7  120.700005  45.0  2.3  11.300000  204.400146   \n",
      "4  58.900002   29.500000 -4.8  109.500000  71.1  3.6  43.000000  189.203964   \n",
      "\n",
      "     q        def       soil    swe  pdsi  vpd   Latitude   Longitude  \n",
      "0  0.0  23.900000  12.599999  1.233  0.81  3.6 -34.027900  150.771000  \n",
      "1  0.0  24.400000  10.700000  0.938  1.28  3.1 -34.821595  147.193697  \n",
      "2  0.0  21.400000   8.099999  0.942  0.78  3.2 -36.617759  146.882941  \n",
      "3  0.0  20.199999   8.000000  0.951  0.70  4.4 -37.470900  144.744000  \n",
      "4  0.0  18.900000   9.900000  1.096  0.50  5.6 -38.400153  145.018560  \n"
     ]
    }
   ],
   "source": [
    "# Load Datasets\n",
    "train_df = pd.read_csv(\"/Users/vinayakshanbhag/Downloads/Training_Data.csv\")\n",
    "val_df = pd.read_csv(\"/Users/vinayakshanbhag/Downloads/Validation_Template.csv\")\n",
    "\n",
    "train_climate = extract_climate(train_df, tiff_path)\n",
    "val_climate = extract_climate(val_df, tiff_path)\n",
    "\n",
    "# Ensure df_climate is correctly processed from the TIFF file\n",
    "print(\"Climate (Training) Data Shape:\", train_climate.shape)\n",
    "print(train_climate.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aaf9c56e-021f-4ceb-87db-9d2822d9eca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Climate Data Shape: (6419, 17)\n",
      "   Latitude  Longitude  Occurrence Status       tmin        tmax  vap  \\\n",
      "0  -34.0279   150.7710                  1  53.500000   65.300003 -4.5   \n",
      "1  -34.8216   147.1937                  1  24.800001  110.800003 -3.9   \n",
      "2  -36.6178   146.8829                  0  51.299999   28.200001 -3.8   \n",
      "3  -37.4709   144.7440                  1  41.000000   67.300003 -4.7   \n",
      "4  -38.4002   145.0186                  1  58.900002   29.500000 -4.8   \n",
      "\n",
      "          ppt  srad   ws        aet         pet    q        def       soil  \\\n",
      "0  115.500000  49.7  2.5  16.700001  200.799149  0.0  23.900000  12.599999   \n",
      "1  143.100006  26.0  1.3   2.500000  217.699799  0.0  24.400000  10.700000   \n",
      "2  115.099998  69.9  3.5  68.800003  204.000031  0.0  21.400000   8.099999   \n",
      "3  120.700005  45.0  2.3  11.300000  204.400146  0.0  20.199999   8.000000   \n",
      "4  109.500000  71.1  3.6  43.000000  189.203964  0.0  18.900000   9.900000   \n",
      "\n",
      "     swe  pdsi  vpd  \n",
      "0  1.233  0.81  3.6  \n",
      "1  0.938  1.28  3.1  \n",
      "2  0.942  0.78  3.2  \n",
      "3  0.951  0.70  4.4  \n",
      "4  1.096  0.50  5.6  \n"
     ]
    }
   ],
   "source": [
    "# Merge datasets to train the model on this datset that contains climate variables and occurences for each latitude and longitude values\n",
    "# Rounding to 4 decimal places so that its easier\n",
    "train_df[\"Latitude\"] = train_df[\"Latitude\"].round(4)\n",
    "train_df[\"Longitude\"] = train_df[\"Longitude\"].round(4)\n",
    "train_climate[\"Latitude\"] = train_climate[\"Latitude\"].round(4)\n",
    "train_climate[\"Longitude\"] = train_climate[\"Longitude\"].round(4)\n",
    "\n",
    "val_df[\"Latitude\"] = val_df[\"Latitude\"].round(4)\n",
    "val_df[\"Longitude\"] = val_df[\"Longitude\"].round(4)\n",
    "val_climate[\"Latitude\"] = val_climate[\"Latitude\"].round(4)\n",
    "val_climate[\"Longitude\"] = val_climate[\"Longitude\"].round(4)\n",
    "\n",
    "# Merge and drop values with null values\n",
    "train_merged = pd.merge(train_df, train_climate, on=[\"Latitude\", \"Longitude\"]).dropna()\n",
    "val_merged = pd.merge(val_df, val_climate, on=[\"Latitude\", \"Longitude\"]).dropna()\n",
    "\n",
    "# Ensure train_merged is correctly processed from the TIFF file\n",
    "print(\"Climate Data Shape:\", train_merged.shape)\n",
    "print(train_merged.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2520eab-9740-47a4-b713-d63389f8f06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier & Missing Handling\n",
    "def remove_outliers_iqr(data, column):\n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower = Q1 - 1.5 * IQR\n",
    "    upper = Q3 + 1.5 * IQR\n",
    "    return data[(data[column] >= lower) & (data[column] <= upper)]\n",
    "\n",
    "def remove_outliers_z(data, column, threshold=3.5):\n",
    "    z_scores = np.abs(zscore(data[column]))\n",
    "    return data[z_scores < threshold]\n",
    "\n",
    "# Apply on selected variables\n",
    "for col in [\"aet\", \"soil\"]:\n",
    "    train_merged = remove_outliers_iqr(train_merged, col)\n",
    "for col in [\"pdsi\", \"vap\", \"ws\"]:\n",
    "    train_merged = remove_outliers_z(train_merged, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ccf9462-f08d-481d-87f4-a629014ba9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop highly correlated variables threshold > 0.9\n",
    "drop_vars = ['pet', 'ppt', 'q', 'vpd']\n",
    "X = train_merged.drop(columns=[\"Occurrence Status\", \"Latitude\", \"Longitude\"] + drop_vars)\n",
    "y = train_merged[\"Occurrence Status\"]\n",
    "\n",
    "# Scale\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_val = val_merged.drop(columns=[\"Latitude\", \"Longitude\"] + drop_vars)\n",
    "X_val_scaled = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb38878f-124c-4328-a295-787d2d12d351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split for local evaluation\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "482f17dd-9076-42a9-bed9-01d7fff0688e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Random Forest Accuracy: 0.7804\n",
      "\n",
      " Confusion Matrix:\n",
      " [[320 151]\n",
      " [109 604]]\n",
      "\n",
      " Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.68      0.71       471\n",
      "           1       0.80      0.85      0.82       713\n",
      "\n",
      "    accuracy                           0.78      1184\n",
      "   macro avg       0.77      0.76      0.77      1184\n",
      "weighted avg       0.78      0.78      0.78      1184\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Model 1: Default RF\n",
    "\n",
    "# Train model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\" Random Forest Accuracy: {accuracy:.4f}\")\n",
    "print(\"\\n Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"\\n Classification Report:\\n\", class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9be1631-ef85-4cd1-9ab8-81ac6c117b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved as: Predicted_RF.csv\n"
     ]
    }
   ],
   "source": [
    "# Predict on validation set\n",
    "val_rfpreds = rf_model.predict(X_val_scaled)\n",
    "val_merged[\"Occurrence Status\"] = val_rfpreds\n",
    "val_merged[[\"Latitude\", \"Longitude\", \"Occurrence Status\"]].to_csv(\"Predicted_RF.csv\", index=False)\n",
    "print(\" Saved as: Predicted_RF.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5814eb9-9ff0-42d4-b7b0-2911edc78a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Tuned XGBoost Performance:\n",
      " Accuracy: 0.768581081081081\n",
      " Classification Report:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.65      0.69       471\n",
      "           1       0.78      0.85      0.82       713\n",
      "\n",
      "    accuracy                           0.77      1184\n",
      "   macro avg       0.76      0.75      0.75      1184\n",
      "weighted avg       0.77      0.77      0.77      1184\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vinayakshanbhag/anaconda3/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [20:07:38] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    }
   ],
   "source": [
    "# Model 2: Tuned XGBoost\n",
    "\n",
    "xgb_model = XGBClassifier(\n",
    "    subsample=1.0,\n",
    "    reg_lambda=5,\n",
    "    reg_alpha=1,\n",
    "    n_estimators=100,\n",
    "    max_depth=8,\n",
    "    learning_rate=0.1,\n",
    "    gamma=0,\n",
    "    colsample_bytree=0.8,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric=\"logloss\",\n",
    "    random_state=42\n",
    ")\n",
    "xgb_model.fit(X_train, y_train)\n",
    "xgb_preds_test = xgb_model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "print(\" Tuned XGBoost Performance:\")\n",
    "print(\" Accuracy:\", accuracy_score(y_test, xgb_preds_test))\n",
    "print(\" Classification Report:\", classification_report(y_test, xgb_preds_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b85023c-9955-4463-8fb0-f15b177d1e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved as: Predicted_XGB_Final.csv\n"
     ]
    }
   ],
   "source": [
    "# Predict on validation set\n",
    "val_preds_xgb = xgb_model.predict(X_val_scaled)\n",
    "val_merged[\"Occurrence Status\"] = val_preds_xgb\n",
    "val_merged[[\"Latitude\", \"Longitude\", \"Occurrence Status\"]].to_csv(\"Predicted_XGB_Final.csv\", index=False)\n",
    "print(\" Saved as: Predicted_XGB_Final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2f5380d-9d46-46a1-9e28-83f1ee5a4db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Tuned 1 Random Forest Performance:\n",
      " Accuracy: 0.777027027027027\n",
      " Classification Report:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.59      0.68       471\n",
      "           1       0.77      0.90      0.83       713\n",
      "\n",
      "    accuracy                           0.78      1184\n",
      "   macro avg       0.78      0.74      0.75      1184\n",
      "weighted avg       0.78      0.78      0.77      1184\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Model 3: Tuned RF\n",
    "\n",
    "# Train model\n",
    "rftuned1_model = RandomForestClassifier(\n",
    "    n_estimators=290,\n",
    "    max_depth=10,\n",
    "    max_features='log2',\n",
    "    min_samples_leaf=2,\n",
    "    min_samples_split=2,\n",
    "    bootstrap=False,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rftuned1_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "rftuned1_preds_test = rftuned1_model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "print(\" Tuned 1 Random Forest Performance:\")\n",
    "print(\" Accuracy:\", accuracy_score(y_test, rftuned1_preds_test))\n",
    "print(\" Classification Report:\", classification_report(y_test, rftuned1_preds_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09487600-95c5-4c01-a5b5-2afb6bd7bcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on validation set\n",
    "val_preds_rf = rftuned1_model.predict(X_val_scaled)\n",
    "val_merged[\"Occurrence Status\"] = val_preds_rf\n",
    "val_merged[[\"Latitude\", \"Longitude\", \"Occurrence Status\"]].to_csv(\"Predicted_RF_Final.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af1486a3-3524-4b52-a22a-40ea9677a1be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      " Tuned Random Forest (via RandomizedSearchCV)\n",
      " Best Hyperparameters: {'bootstrap': True, 'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 10, 'n_estimators': 174}\n",
      "\n",
      " Accuracy: 0.7838\n",
      "\n",
      " Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.68      0.71       471\n",
      "           1       0.80      0.86      0.83       713\n",
      "\n",
      "    accuracy                           0.78      1184\n",
      "   macro avg       0.78      0.77      0.77      1184\n",
      "weighted avg       0.78      0.78      0.78      1184\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Model 4: Tuned RF with RandomizedSearchCV\n",
    "\n",
    "# Define parameter grid for tuning\n",
    "param_dist = {\n",
    "    'n_estimators': randint(100, 300),\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Initialize the base model\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Perform randomized search\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=rf,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=50,\n",
    "    scoring='f1',\n",
    "    cv=3,\n",
    "    verbose=2,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit on your cleaned training data (X_train_scaled and y_train)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Extract the best model\n",
    "best_rf = random_search.best_estimator_\n",
    "\n",
    "# Predict on holdout test set\n",
    "rf_test_preds = best_rf.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "print(\" Tuned Random Forest (via RandomizedSearchCV)\")\n",
    "print(\" Best Hyperparameters:\", random_search.best_params_)\n",
    "accuracy = accuracy_score(y_test, rf_test_preds)\n",
    "print(f\"\\n Accuracy: {accuracy:.4f}\")\n",
    "print(\"\\n Classification Report:\")\n",
    "print(classification_report(y_test, rf_test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8f96599-1ad1-4baa-a758-0b212dcd57b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on validation set\n",
    "val_rf_preds = best_rf.predict(X_val_scaled)\n",
    "val_merged[\"Occurrence Status\"] = val_rf_preds\n",
    "val_merged[[\"Latitude\", \"Longitude\", \"Occurrence Status\"]].to_csv(\"Predicted_Data_RF_Tuned_CV_.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f39ca6e-7967-48e7-8c7d-c856b482d498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vinayakshanbhag/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:425: FitFailedWarning: \n",
      "70 fits failed out of a total of 250.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "38 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/vinayakshanbhag/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/vinayakshanbhag/anaconda3/lib/python3.11/site-packages/sklearn/base.py\", line 1144, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/Users/vinayakshanbhag/anaconda3/lib/python3.11/site-packages/sklearn/base.py\", line 637, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/Users/vinayakshanbhag/anaconda3/lib/python3.11/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "32 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/vinayakshanbhag/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/vinayakshanbhag/anaconda3/lib/python3.11/site-packages/sklearn/base.py\", line 1144, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/Users/vinayakshanbhag/anaconda3/lib/python3.11/site-packages/sklearn/base.py\", line 637, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/Users/vinayakshanbhag/anaconda3/lib/python3.11/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/vinayakshanbhag/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_search.py:976: UserWarning: One or more of the test scores are non-finite: [0.80292462        nan 0.80689522 0.79982252 0.8004513  0.80047672\n",
      "        nan        nan 0.80407052 0.8010789         nan 0.80950703\n",
      " 0.80151402 0.80461823        nan 0.80593983 0.80041882 0.80697562\n",
      "        nan        nan 0.80082246 0.80381186        nan 0.80665945\n",
      " 0.80452405 0.80265129 0.80111044 0.80904445 0.80778665 0.81249168\n",
      "        nan 0.80442022 0.79999948 0.80606038        nan 0.80098035\n",
      "        nan 0.80397557 0.79868967        nan 0.80301333 0.80368357\n",
      " 0.8110704  0.80010726        nan 0.80199009        nan 0.80481385\n",
      " 0.80225304 0.80101728]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best Parameters: {'bootstrap': True, 'max_depth': 25, 'max_features': 'log2', 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 480}\n",
      " Accuracy: 0.7829391891891891\n",
      " Classification Report:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.71      0.72       471\n",
      "           1       0.81      0.83      0.82       713\n",
      "\n",
      "    accuracy                           0.78      1184\n",
      "   macro avg       0.77      0.77      0.77      1184\n",
      "weighted avg       0.78      0.78      0.78      1184\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Model 5: RF Advancedtuning + Expanded Hyperparameter Grid + StratifiedKFold + Class Weights (Best Model)\n",
    "\n",
    "# Define enhanced parameter grid\n",
    "param_dist = {\n",
    "    'n_estimators': randint(150, 500),\n",
    "    'max_depth': [None, 10, 15, 20, 25, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2', 'auto'],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Use Stratified K-Fold for CV\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize RF with class weights\n",
    "rf = RandomForestClassifier(class_weight=\"balanced\", random_state=42)\n",
    "\n",
    "# Randomized Search CV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=rf,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=50,\n",
    "    scoring='f1',\n",
    "    cv=cv,\n",
    "    verbose=2,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit\n",
    "random_search.fit(X_train, y_train)\n",
    "best_rf_advanced = random_search.best_estimator_\n",
    "\n",
    "# Evaluate\n",
    "test_preds = best_rf_advanced.predict(X_test)\n",
    "print(\" Best Parameters:\", random_search.best_params_)\n",
    "print(\" Accuracy:\", accuracy_score(y_test, test_preds))\n",
    "print(\" Classification Report:\", classification_report(y_test, test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47843f8d-5ba3-4763-88bc-31253e157abe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved as: Predicted_RF_Tuned_Advanced.csv\n"
     ]
    }
   ],
   "source": [
    "# Predict on validation\n",
    "val_preds = best_rf_advanced.predict(X_val_scaled)\n",
    "val_merged[\"Occurrence Status\"] = val_preds\n",
    "val_merged[[\"Latitude\", \"Longitude\", \"Occurrence Status\"]].to_csv(\"Predicted_RF_Tuned_Advanced.csv\", index=False)\n",
    "print(\" Saved as: Predicted_RF_Tuned_Advanced.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "383e859a-2f1a-4552-b67f-76c91822ae97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      " Best Parameters: {'bootstrap': True, 'max_depth': 15, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 316}\n",
      " Accuracy: 0.7837837837837838\n",
      " Classification Report:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.68      0.72       471\n",
      "           1       0.80      0.85      0.83       713\n",
      "\n",
      "    accuracy                           0.78      1184\n",
      "   macro avg       0.78      0.77      0.77      1184\n",
      "weighted avg       0.78      0.78      0.78      1184\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define enhanced parameter grid\n",
    "param_dist = {\n",
    "    'n_estimators': randint(150, 500),\n",
    "    'max_depth': [None, 10, 15, 20, 25, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Use Stratified K-Fold for CV\n",
    "cvs = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize RF with class weights\n",
    "rf1 = RandomForestClassifier(class_weight=\"balanced\", random_state=42)\n",
    "\n",
    "# Randomized Search CV\n",
    "random_search1 = RandomizedSearchCV(\n",
    "    estimator=rf,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=50,\n",
    "    scoring='f1',\n",
    "    cv=cv,\n",
    "    verbose=2,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit\n",
    "random_search1.fit(X_train, y_train)\n",
    "best_rf_advanced1 = random_search1.best_estimator_\n",
    "\n",
    "# Evaluate\n",
    "test_preds1 = best_rf_advanced1.predict(X_test)\n",
    "print(\" Best Parameters:\", random_search1.best_params_)\n",
    "print(\" Accuracy:\", accuracy_score(y_test, test_preds1))\n",
    "print(\" Classification Report:\", classification_report(y_test, test_preds1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a5259beb-59c9-42d7-8d06-6f45e75e05eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved as: Predicted_RF_Tuned_xyz.csv\n"
     ]
    }
   ],
   "source": [
    "# Predict on validation\n",
    "val_preds1 = best_rf_advanced1.predict(X_val_scaled)\n",
    "val_merged[\"Occurrence Status\"] = val_preds1\n",
    "val_merged[[\"Latitude\", \"Longitude\", \"Occurrence Status\"]].to_csv(\"Predicted_RF_Tuned_xyz.csv\", index=False)\n",
    "print(\" Saved as: Predicted_RF_Tuned_xyz.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e30697c3-c904-4c8e-993e-74e83a12946b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_merged.to_csv(\"Training_Data_Climate_Merged.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce8f650-eb10-40ac-9943-285dd7f159b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
